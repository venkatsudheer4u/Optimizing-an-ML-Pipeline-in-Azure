# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
This dataset given in the project deals with the Marketing data about Individuals.
The data collected was from phone calls. it aims at classifying whether the person will subscribe to the bank term deposit (yes or no).

The best performing model was from Hyperdrive with an accuracy of 91.76%, AutoML also did well in getting accuracy close to the model from Hyperdrive with an accuracy of 91.71%

Model details :

Hyperdrive model : HD_4986f363-fe92-4c46-a60c-da7ddb555c03
AutoML model : AutoML_5c1d61cd-f114-40c6-89e1-06d455502cad

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
**What are the benefits of the parameter sampler you chose?**
The parametersampler I have choosen was as below :
ps = RandomParameterSampling(
    {
        '--C' : choice(0.001,0.01,0.1,1,10,20,25,50,100,200,300,500,1000),
        '--max_iter': choice(50,100,200,250)
    }
)

In the above code : C is for Regularization and max_iter is for the max. number of iterations. Random parameter sampler is faster and can assign early termination of low performance runs.

**What are the benefits of the early stopping policy you chose?**
Early stopping is a process where the runs with poor performance are automatically terminated there by improving efficiency.
The policy I have used was mentioned below :

policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)

Evaluation iterval is a value given to specify the frequency of applying the early stopping policy.
slack factor : The amount of slack allowed to the best performing training run. it is a ratio.

### About the updated Run details regarding Hyperdrive Runs.
### As an update for the feedback received on RunDetails. 
I have attached image by running the project again. saved the details as an Image.
but it did not complete the AutoML part due to disconnected portal. Please consider the latest image added (Run_details_added.PNG)
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML Configuration used :

automl_config = AutoMLConfig(
    compute_target = cpu_cluster,
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='accuracy',
    training_data=ds,
    label_column_name='y',
    n_cross_validations=2)

"Experiment timeout" is the time in minutes to exit the experiment. 
"task" is nothing but the type of problem we are solving, say classification or Regression
"primary Metric" is the metric used for evaluation of the model.
cross validations says the number of cross validations to perform.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Hyperdrive best model : (with C 25 and max iter 100)
Accuracy : 91.76%

AutoML best model : VotingEnsemble
Accuracy : 91.71%

I could see that there is a slightly higher accuracy in Hyperdrive model, however the difference is very small.But if we consider the AUC_weighted metric from AutoML model, we could see 0.946 which is a better metric for imbalanced datasets.The effort we put towards running an AutoML was very less but could get fairly near value to Hyperdrive model and better result in terms of AUC weighted shows the advantage of AutoML.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
One observation made was about dataset which is highly imbalanced.
Of the total 32950 value counts of target column, 29528 was Yes and remaining was no.
which is like 90% to 10%.
we could deal this with different methods:
Under Sampling of the Majority class
Over Sampling of Minority class
imblearn package can be used.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
I have used the code in the last cell :
cpu_cluster.delete()
image is placed in the image folder.
